% TODO: 
% * Mention rdtsc() as replacement for a clock DONE
% * Update run and compile to return a success or failure value DONE
% * Clarify in case study: entropy parameter is only an illusrative artificial parameter to create randomness in measurements DONE
% * Update data format description in data.m UNREQUIRED

\section{Introduction}
Measuring and representing the performance of an algorithm represents quite a tedious task. Such activity requires a constant monitoring and intervention to collect data from an experiment, start the next one and check if everything works as expected. Such constant attention is error-prone. Once the data has been fully collected, it is often cumbersome and also error-prone to reshape it to a meaningful format such as a graph, or a format allowing the computation of such graph. There are many benefits in automatizing this process: making it autonomous, more reliable or faster.

The document introduces here a very simplified case study, where it is proposed to demonstrate the scalability of a multi-threaded program whose ``computation'' consists in achieving a given number of jumps through a loop. The sequential version takes the complete loop sequentially whereas the parallel implementation splits this loops in smaller chunks and distributes it equally among all threads. The performance of this program is monitored and plotted following a workflow and using Freja and R or Octave/Matlab, that are introduced along the solving of this case study. The language used in this work is {C}, but any other programming language should work equally well. The data can be process using either R, an open source ``system for statistical computation and graphics''~\cite{hornik14} or Octave/Matlab. Although R is preferred for data analysis and graph production, users familar with Matlab scripting can use Matlab, or its open-source implementation Octave as well as a set of data analysis primitives described in this document, to produce meaningful representations of the data produced. Section~\ref{sec:files} lists and briefly describes each files provided by Freja. Section~\ref{sec:problem} introduces the problem that illustrates this case study. Section~\ref{sec:process} details all the necessary steps to instrument a program, collect the results and use it to generate comprehensive representations. Finally, section~\ref{sec:conclusion} concludes the document.

%\section{Motivations}
%Measuring the performance of a computer program can be a very tedious task to do. Quite often, this involves the comparison of several different combinations of settings (such as the number of thread of memory allocate to the job) in order to observe how they affect the behavior of this program. Also, since many parameters can affect the performance to be measured, such as the underlying scheduler of the operating system, it is often a good practice to run each setting several times and extract statistical data rather to raw performance number. A high number of settings, combined with the necessity of running each program variants several times generates a high number of executions to supervise and makes it often difficult, if not impossible, to do it manually. An human operator can easily forget to change a parameter between two experiments, due to weariness or tiredness; as a consequence, the more an experiment contains settings to be manually changed, the higher is the risk of mistake at random steps of this experiments. In contrast, an automated process, when designed correctly, can repeat these tasks with no weariness nor tiredness and is much less subject to such errors.

%However, such automated process is difficult to design correctly. Possible reasons for these difficulties include bugs in the implementation of the automated process, but also a preference to automated experimental setup specifically targeted from their early development phases, to the observation of particular phenomenons the experiments aims to emphasize. Such strategy may shorten the development of the necessary scripting tools, but it has several drawbacks. It often results in the generation of semi human friendly data format, where data is shown in a semi-structural form surrounded by comments. Such formats are not easy to understand for humans, and require specific parsing tools to automatically gather fully structured data and generate visual representations of them. Because the tools are specifically developed for each experiments, outputs from different experiments are likely to have a different structure. This results in the necessity to rewrite the output parsers from one experiment to another and thus requires more efforts in the longer run. Moreover, because the data is collected aiming at the emphasize of a particular phenomenon, is is also often hard to reuse this data to show different properties. As a consequence it is necessary to design a new specific automated process, incompatible with other already existing tools because of too specific their characteristics. This results in even more necessary efforts to design and use an automated process in experimentation. Finally, a myriad of automated scripts quickly become cumbersome to use, because of their heterogeneous design resulting in heterogeneous user interfaces and workflows.

%Automated tools and a suitable workflow reduces these issues. It simplifies and unifies the process to automatically gather, structure and represent data collected through the performance measurements of as many program variants as the user and resources can afford to build and run automatically. here, the flow consists in several steps, from instrumentation of algorithm to data representation through data collection and processing. The method and tools are designed to be as general as possible, pushing specific steps as far as possible toward the instrumentation (early stage) and processing (late stage). In between these extreme, the data output from experiments take the same fully structured form of a numeric matrix, directly usable by scientific tools such as Matlab or its open-source competitor Octave. Such records are easily browsed through methods inspired from relational algebra and widely used in relational databases. This allows an easy extraction and transformation of interesting information, regardless of what the experiment specifically aimed at monitoring (provided that all necessary information was recorded). This information can be used to compute more simple matrices that can be directly represented in a visual form, using tools Matlab or Octave provide or more simple and complete handlers provided in the workflow and also described in this document.

\section{Requirements}
The user of the tools and workflow described in this document is expected to be acquainted with a number of technologies that are conjointly exploited in the flow. This includes basic knowledge of Bash, a confident use of Matlab or Octave scripting and notions in relational algebra or experience in databases
%\footnote{TDDD46 aims: Design and use a relational database, explain the theory behind the relational model and how this affects good design of databases.}
. Debugging or developing the tools requires extensive knowledge in Bash, R and/or Matlab/Octave scripts. A sharp mastery of good programming practices is always most welcome. The reader can find introductory documentation on the internet about Bash~\cite{mikeg00,garrels08}, Make~\cite{anonymous_make_1,gnu_make}, R~\cite{r,rdebuts,ggplot2,ggplot2_examples}, Matlab scripts~\cite{huber97} and functions~\cite{recktenw95} (also applicable to Octave\footnote{http://www.gnu.org/software/octave/}), the C programming language~\cite{cprogramming} and Posix thread programming~\cite{tim10,barney12}.

\section{Toolset}
All the flow described in this document relies of tools such as Freja~\cite{freja} for data generation, itself relying heavily on Bash~\cite{garrels08}. Data analysis is performed by GNU R~\cite{r} for data analysis and representation. The user can also use Matlab or Octave, together with data analysis primitives introduced along this document. A typical project can also benefit from GNU make~\cite{gnu_make} and good programing skills in the language of the reader's choice. Beside the experimental program and its building toolchain, a typical experimental project with Freja includes the following files:
%a set of scripts, composed of the files in the following list:
\begin{itemize}
%\item start\\
%Starts the compilation process or runs all experiments in one, unsupervised batch. Start it explicitly with bash (\emph{bash start ...}) instead of running it directly (incorrect: \emph{./start}). This file does not need to be modified under normal use, but only for contribution purpose. Refer to the sections \ref{sec:compile} and \ref{sec:run} more more information about it.
\item variables\\
Contains all the settings the program can take and their possible values. The user must adapt it for each different program or experiments. These values are read by \emph{Freja} when compiling or running program variants. See Sec.~\ref{sec:plan} to get more information about this file. 
\item compile\\
Compile a variant of the program to be monitored. This script is called by \emph{Freja} and is given as parameters all settings defined as compilation settings in the \emph{variables} file (see above) as well as one value for each of them, in the same order as defined in the field ``compiled'' of the file \emph{variables}. This script typically calls make with the corresponding options to build the correct program variant. The user must adapt it to the program to be monitored.
\item run\\
Runs a variant of the program to be monitored. It is called by \emph{Freja} with compile settings first then run settings and their associated value for one program variant, given as arguments.
%\item settings\\
%Various settings read by \emph{start} and that slightly affect its behavior. This files should not be modified nor even explicitly used.
%\item merge\\
%Merges the result of two experiments whose difference is not described in \emph{variables}. These two experiments must have output a result matrix having the same size and the same meaning for each of its columns. This script makes use of \emph{merge.m} and requires octave to work. Matlab cannot be used as an alternative.
%\item data.m, data-xxxxx-yyyyyyy.m\\
%Contains the raw values collected from all experiments and program variants \emph{start} compiled and ran. This file is automatically generated when \emph{bash start run <name>} finishes and should not be manually modified.
\item drawplots.r\\
Experiment-specific R script that reads the data produced by Freja in csv format, and produces one or several output graphs. This script is entirely experiment-specific and needs to be rewritten for each experiment.
\item drawplots.m\\
Experiment-specific Octave or Matlab script that reads the data produced by Freja and converted to a format Matlab and Octave can load. This script transforms this data into smaller, ready to plot matrices. This file should be entirely re-written for all program to monitor.
\item *.m
Freja-provided Matlab/Octave primitives inspired from relational algebra that facilitate data manipulation.
%\item octave/*.m, Matlab/*.m\\
%Respectively Octave and Matlab specializations of data manipulation scripts, used by \emph{plot\_data.m}. Copy them into the same directory as the raw result file \emph{data.m} and run plot\_data.m from this directory (\emph{octave plot\_data.m} from the same directory, using Octave).
\end{itemize}

\section{The problem}
\label{sec:problem}
% Introduce the computation problem to be monitored and the way it has been paralellized
Suppose an algorithm that achieves some work in a sequential fashion, for instance the code shown in Listing.\ref{lst:do_some_work}. We want to optimize the running of this implementation through parallelization. We need to compare the performance of both the sequential version and its parallel implementation under various conditions in order to deduce speedup achieved by parallelization as well as the conditions in which parallelization provides, or doesn't provide, any improvement. The parallelization of this algorithm is achieved through the division of $ct$ by the number of threads employed, and every thread runs the loop with this new value of $ct$. As this example is very simple, its performance variate very little from run to run. In real high-performance computing experiments, many uncontrollable factors influence the performance of a program in a seemingly random way. In order to simulate this effect, the scenario depict in this document variates randomly the number of jumps $ct$ with an entropy defined by the variable $entropy$.
%Since this example is very simple, the performance measurement reveal few differences from run to run. Not only this is usually not the case, but it also prevent the illustration of the standard deviation plotting capability. Therefore $ct$ variates more or less, randomly, along another property named $entropy$, which models this missing performance randomness from run to run.
The parallel implementation must show good scaling properties with the number of cores employed, therefore demonstrating an efficient use of them. This requires the computation load to be equally spread among the cores available; in other words, all cores must compute for a roughly equivalent period of time. Both global and per-core computation time must be recorded and shown in a representation similar to Fig.\ref{fig:timing-200}. In this work, we consider threads and cores as equivalent. Figure~\ref{fig:timing-200} represents the global computation time (gray curve) as a function of number of threads, as well as individual threads' computation time for every situation, using the sequential algorithm or its parallel version using one to eight threads (the parallel version with one thread shows the overhead of parallelization). Both sequential and parallel implementations are available in appendices~\ref{app:sequential} and~\ref{app:parallel}, respectively. These implementations introduce 3 settings: the number of threads to use (degree of parallelism), the number of jumps the program must do (that models the size of the problem) and the entropy in performance described above. Each of these settings take their value from $\mathbb{N}$, $\mathbb{R}$ and $\left[0;1\right]$, respectively. Consequently, all variants of the algorithm are contained in a 3 dimensions space. Section~\ref{sec:process} explain the process to obtain a figure such as Fig.\ref{fig:timing-200}.

\begin{figure}
\centering
\includegraphics[width=10cm]{images/timing-200.eps}
\caption{Example of a plot obtained through the measure and plotting scripts set.}
\label{fig:timing-200}
\end{figure}

\begin{lstlisting}[caption={``Computation''-intensive algorithm used as case study in this tutorial.},label={lst:do_some_work},language=C]
static int
do_some_work(unsigned long long int ct)
{
  unsigned long long int i;

  // Bring it on, yeah!
  for (i = 0; i < count; i++);

  return 0;
}
\end{lstlisting}

\section{Setting up an experiment, collecting, extracting and plotting data}
\label{sec:process}
Sections~\ref{sec:setup} through \ref{sec:format} describe all the necessary steps to take in order to safely monitor a program and generate a visual representation of the data gathered. All these steps take as case study the problem described in section~\ref{sec:problem}.

\subsection{Step 1: Set up your program}
\label{sec:setup}
%The effect of one or more settings on the general behavior and performance of an algorithm can be measured by measuring and comparing the performance of all variants defined by these settings. All differences between every variants not induced by these settings must be minimized as much as possible, so that only the differences these settings generate are measured. Considering an implementation of an algorithm always may need to be updated, keeping several copies of code with slight differences makes such maintenance difficult. At every necessary changes, the programmer has to update correctly every copies of the code and may forget one copy and/or commit a mistake while reporting the change in another copy. Such methodology may generate numerous copies to be maintained, which number grows exponentially with the number of settings and the number of value these settings can take. In addition, having one copy for each possible setting combinations makes difficult to obtain a new variant for which no additional copy of the code was dedicated.

%Instead of maintaining several copies for each setting to be tested, it is more advisable alter limited portions of a unique copy of the code, when compiling or running each variants. This can be done in two ways: either by giving the program an argument at runtime, either using preprocessor directives in the code that the compiler can interpret. Using preprocessor directives, the compiler can limit the compilation only to the relevant portions of code and using the right constant values, according to options given when compiling. This case studies illustrates argument passing to provide the size of the problem to be computed, as well as preprocessor directive to define both if the algorithm must be sequential or parallel (and in this case the number of threads to use) and the degree of entropy expected when measuring the performance.

Modify your source code to integrate several versions (sequential and parallel) to the same source file. A version or another is chosen through preprocessor directives (\#if, \#ifdef, \#ifndef, etc) at compilation time. Each directive takes a decision according to the value of definitions (defined by the directive \#define or passed by the compiler). A definition represents a \emph{setting} and takes one value among several possible. Use directives and definitions to select portions of codes that are specific to a program version, but keep these specific portions as small as possible. Figure~\ref{lst:directive} provides a simplified illustration of this technique; you can also find more example and information on the internet. Appendix~\ref{app:unified} shows the sequential and parallel version of the same algorithm merged into a single source file. Depending on the options given to the compiler (-D switch with gcc, see gcc --help), the sequential or a parallel version is compiled. The way the relevant options are given to the compiler is described in section~\ref{sec:compile}.

Another way to control the behavior of a program among different versions are arguments to the program when it is started. Depending on the values in argv, the program can branch to one or another portion of code. However, since this decision is taken at runtime, it can slow down the execution time of the program. This should be avoided if the execution time of the program is monitored.
%In {C}, such preprocessor directives are {\#if + condition over a constant symbol}, {\#ifdef + constant symbol} or its complement {\#ifndef + constant symbol}, respectively the following code if the condition is achieved when compiling the code, if the constant symbol is defined when compiling or if it is undefined at compilation time. The in Fig.\ref{fig:directive}, constant symbol is {NB\_THREADS}. All these preprocessor directives can be complemented using the {\#else} directive and must be ended by {\#endif} after the code which compilation is decided by this directive.

\begin{lstlisting}[caption={If the constant {NB\_THREAD} is defined to a value greater than zero when compiling this code, then a multi-threaded version of the code is compiled, using pthreads. Otherwise the sequential version is generated, that simply calls the function achieving the computation.},label={lst:directive},language=C]
#if NB_THREADS > 0
  pthread_t thread[NB_THREADS];
  pthread_attr_t attr;
  thread_do_some_work_arg arg[NB_THREADS];
  int i;

  pthread_attr_init(&attr);
  for (i = 0; i < NB_THREADS; i++)
    {
      arg[i].id = i;
      // influence the number of loops to
      // achieve by the value of entropy
      // to simulate performance randomness
      arg[i].count = variate(count, entropy);

      pthread_create(&thread[i], &attr,\
 thread_do_some_work, (void*) &arg[i]);
    }

  for (i = 0; i < NB_THREADS; i++)
    {
      pthread_join(thread[i], NULL);
    }
#else
  count = variate(count, entropy);
  do_some_work(count);
#endif
\end{lstlisting}

\subsection{Step 2: Monitor the performance}
\label{sec:instrument}
% The methodology can work with any measure relevant for the experiment, as long as it can be represented with a numeric value (either integer or float). Algorithms' performance can be measured through many dimensions, but a common metric is execution time. A {C} program can use functions such as $clock\_gettime()$ to measure the time with a precision up to the nanosecond. Refer to the documentation (man clock\_gettime) for more information. In this case study, $clock\_gettime()$ is used to measure the time when the algorithm or a thread starts and the time it stops. The difference between these two time stamps is the time the implementation took to run or a thread consumed to perform its task. $clock\_gettime()$ does not provide a unique number but two integers, denoting seconds and the second nanoseconds. Consequently, it is difficult to use this data in this format in the program code without a few calculations that can hinder the program's performance. Instead, this calculation is achieved in the processing phase (see section~\ref{sec:format}), where the performance do not alter the measurement. $clock\_gettime()$ gives the user the choice between five clocks, each having different properties. The following three clocks may be the most useful:
Record the values you want to measure into variables, and display these variables at the end of the program execution, or at a moment when outputting these values does not affect the values you want to measure. Collecting data generally affects execution speed. If you want to measure execution speed, remember that I/o operations such as printf() are very slow and will affect the performance if it is used repeatedly in the code you want to measure. Consider issues such as data locality and try to make as much profit of caches. However, leave a most of the resources available to your program, as using them all for measurement this might significantly slow it down.

\subsection{Measure the time}
There are several functions provided by {C} and the Linux kernel to measure execution time. In general, a good resolution is better. clock\_gettime() allows the measurement of time with a precision of a nanosecond. The downside of this function is the format of this information, that takes shape in two integers, one counts seconds, the second nanoseconds. Such separate format makes difficult the fast calculation of interesting values. Later sections show that this can be done later in the process. Refer to man pages of clock\_gettime() to get more information about it (type ``man clock\_gettime'' in a Linux terminal). It can use several clocks, each of them having different properties. Below is a short description of three interesting clocks:

\begin{enumerate}
\item CLOCK\_THREAD\_CPUTIME\_ID\\
Computation time consumed by a thread. It does not take into account the time when the thread has been scheduled or blocked at a synchronization primitive. Consequently, it does not necessarily represent the time period between when the thread has been started and stopped. The use of this clock is safe to measure the thread execution time only when there is no synchronization involved in this thread and no more threads are used than the underlying computer has cores unused by any other thread in the system.
\item CLOCK\_PROCESS\_CPUTIME\_ID\\
Computation time consumed by the process. This is the time matching the sum of cycles consumed in parallel by all threads in the process. This time is not the time between the start and stop times of a process, equivalent to the execution time of its slowest thread. A process running two threads in parallel for 2 seconds is considered as having consumed 4 seconds of computation.
\item CLOCK\_MONOTONIC\\
System-wise clock, that is shared between every threads. CLOCK\_MONOTONIC provides a common time reference to every process or thread running in the system. This clock does not accumulate the computation time provided by parallel threads (as CLOCK\_PROCESS\_CPUTIME\_ID does), but it does not pause when a thread is scheduled or blocked (as CLOCK\_THREAD\_CPUTIME\_ID). It also count the time other programs running in the system consume. This is the preferred clock to measure time intervals if the computer is not already running an other computation-intensive program.
\end{enumerate}
%The choice of a correct clock is important to obtain the correct information when measuring performances. Appendix~\ref{app:clock} illustrates different results from running the same program compiled and run with the same parameters, except the choice of the clock used to monitor the performances. The reason why such difference arisen may not be obvious and can even interpreted as correct if not enough care is given to the correctness of the data collection method.

In order to calculate the time taken to execute a portion of code, $clock\_gettime()$ is called before and after each computation step being monitored. The difference between the two dates collected gives the execution time of the code between the first and the second collect of time. As described in section~\ref{sec:setup}, these instructions should be protected by preprocessor directives, making easy to recompile and run the program with no instrumentation instructions. Figure~\ref{lst:instrumentation} provides an example of such instrumentation and appendix~\ref{app:instrumented} provides a fully instrumented version of the {C} program.

\begin{lstlisting}[caption={$clock\_gettime()$ before and after the portion of code being monitored collects the necessary data to compute the time used to run this portion of a program.},label={lst:instrumentation},language=C]
#ifdef MEASURE
  clock_gettime(CLOCK_MONOTONIC, &start);
#endif

  srandom(time(NULL));
  count = atoi(argv[1]);
  do_work(count, ENTROPY);

#ifdef MEASURE
  clock_gettime(CLOCK_MONOTONIC, &stop);

#if NB_THREADS > 0
  for (i = 0; i < NB_THREADS; i++)
    {
      printf("%i %i %li %i %li %i %li %i %li\n", i + 1,
          (int) start.tv_sec, start.tv_nsec, (int) stop.tv_sec,
          stop.tv_nsec, (int) thread_start[i].tv_sec,
          thread_start[i].tv_nsec, (int) thread_stop[i].tv_sec,
          thread_stop[i].tv_nsec);
    }
#else
  printf("%i %i %li %i %li %i %li %i %li\n", 0,
      (int)start.tv_sec, start.tv_nsec, (int)stop.tv_sec,
      stop.tv_nsec, (int)thread_start.tv_sec,
      thread_start.tv_nsec, (int)thread_stop.tv_sec,
      thread_stop.tv_nsec);
#endif
#endif
R
  // Always report a successful experiment
  return 0;
}
\end{lstlisting}

\subsubsection{Choosing the right clock}
\label{app:clock}

In figs.~\ref{fig:monotonic} and~\ref{fig:thread}, the same program has been compiled using the same settings, run on the same machine with the same input. Each variant has been run the same amount of time and the same script has been use to generate the plots. The only difference lies in the clock that has been used to measure the performance. The machine on which these two experiments have been run offers two cores. The difference in the measurement starts with the third core: in Listing~\ref{lst:monotonic} and Fig.~\ref{fig:monotonic}, the clock used to measure the execution time is global, and measures the real time\footnote{``real time'' here is not to be confused with real-time computing and denote the time as it flows for the programmer watching his program running.}. The clock used in Listing.\ref{fig:thread} considers the time as the number of processor clock ticks that was given to threads, witch is not necessarily real time. As the machine only offers 2 cores, a third thread has to share its core with another thread, thus making CLOCK\_THREAD\_CPUTIME\_ID unsynchronized with real time (Fig.~\ref{fig:thread}). Notice one can use other processor or OS-provided functions to measure time. As an example, $rdtsc()$ takes profit of registers available on x86 archtectures from Pentium and returns on 64 bits the number of clock cycles elapsed since last processor reset. Figure~\ref{lst:rdtsc} shows an implementation taking profit of the relevant register in a function a regular C program can use.


\begin{lstlisting}[caption={Code using CLOCK\_MONOTONIC to monitor time.},label={lst:monotonic},language=C]
static void*
thread_do_some_work(void* arg)
{
  thread_do_some_work_arg *args;
  args = (thread_do_some_work_arg*) arg;

#ifdef MEASURE
  clock_gettime(CLOCK_MONOTONIC,\
 &thread_start[args->id]);
#endif

  do_some_work(args->count);

#ifdef MEASURE
  clock_gettime(CLOCK_MONOTONIC,\
 &thread_stop[args->id]);
#endif

  return NULL;
}
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=10cm]{images/timing-200-nicolass-monotonic.eps}
\caption{Performance measured using CLOCK\_MONOTONIC.}
\label{fig:monotonic}
\end{figure}

\begin{lstlisting}[caption={Code using CLOCK\_THREAD\_CPUTIME\_ID to monitor time.},label={lst:thread},language=C]
static void*
thread_do_some_work(void* arg)
{
  thread_do_some_work_arg *args;
  args = (thread_do_some_work_arg*) arg;

#ifdef MEASURE
  clock_gettime(CLOCK_THREAD_CPUTIME_ID,\
 &thread_start[args->id]);
#endif

  do_some_work(args->count);

#ifdef MEASURE
  clock_gettime(CLOCK_THREAD_CPUTIME_ID,\
 &thread_stop[args->id]);
#endif

  return NULL;
}
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=10cm]{images/timing-200-nicolass-thread.eps}
\caption{Performance measured using CLOCK\_THREAD\_CPUTIME\_ID.}
\label{fig:thread}
\end{figure}

\begin{lstlisting}[caption={C inline assembly implementation of rdtsc using hardware registers.},label={lst:rdtsc},language=C]
#ifdef __i386
extern __inline__ uint64_t rdtsc() {
  uint64_t x;
  __asm__ volatile ("rdtsc" : "=A" (x));
  return x;
}
#elif defined __amd64
extern __inline__ uint64_t rdtsc() {
  uint64_t a, d;
  __asm__ volatile ("rdtsc" : "=a" (a), "=d" (d));
  return (d<<32) | a;
}
#endif
\end{lstlisting}

\subsubsection{Output the data collected}
The program whose performance is monitored must report the data in a specific way, that allows the expression of any numeric data measured. This data must take the shape of a matrix, which is represented by its values separated by a space, as shown in figure~\ref{lst:output}. In this matrix, each column values of a feature measured, or information related to the values shown by another column (such as the thread id corresponding to a thread's execution time). A line represents a feature's instance. Note that values are not necessarily bound to numeric values; however, non-numeric values must be surrounded by double quotes and should not other double quotes. The use of space is discouraged, especially if the data is to be processed by Matlab or Octave.

Listing~\ref{lst:output} shows the output of the instrumented program of this case study. The first column represents the thread number (there are 8 threads used in this experiment), the second column is the time in seconds when the algorithm was started and the third is the time in nanoseconds (added to the seconds of the second column) when the program was started. The next two columns represent in the same format the time when the program stopped. The four next values represent the start and stop time for every thread. Note that the four columns from 2 to 5 are identical in every line, as this information is global to the program and not thread-specific. Columns 6 to 9 are different in each line, since each of them show the execution time of one thread. The reason why global data is repeated is because the format of a matrix \emph{must} be preserved, in order to be parsed in further steps. In addition, all values must be numeric, integer or decimal.

\begin{lstlisting}[caption={Format the data must take when output from an instrumented program.},label={lst:output},basicstyle=\ttfamily\scriptsize]
1 14221 440637013 14221 445577841 14221 440841139 14221 445505263 "parallel"
2 14221 440637013 14221 445577841 14221 440834267 14221 445373429 "parallel"
3 14221 440637013 14221 445577841 14221 440857615 14221 445338580 "parallel"
4 14221 440637013 14221 445577841 14221 443540848 14221 445500748 "parallel"
5 14221 440637013 14221 445577841 14221 441022065 14221 443081924 "parallel"
6 14221 440637013 14221 445577841 14221 441046838 14221 443176118 "parallel"
7 14221 440637013 14221 445577841 14221 441074851 14221 443380452 "parallel"
8 14221 440637013 14221 445577841 14221 441109564 14221 443507215 "parallel"
\end{lstlisting}

Once data is produced or in case of any failure, it is important to exit the program with a relevant exit value (\emph{return 0}). Returning 0 reports a run to work as expected and output to be taken into account. A return value different than 0 denotes a unsuccessful run and will make the batch to discard output produced. See section~\ref{sec:run} for more details about return values and the way they are handled. In this scenario, we simulate program failures by making our program to return a random value between 0 and 5, as illustrated in Listing~\ref{lst:failure}.

\begin{lstlisting}[caption={Random return value to simulate program failures.},label={lst:failure},language=C]
srand(time(NULL));
return !(random() % 6);
\end{lstlisting}

%The collected data must be forwarded to the scripts managing the experiment through I/O:s, usually by writing them on the screen or to a file. Such operation must be performed with caution, as I/O:s are very slow and may affect the performance measurement. This can happen when some threads output the data they collected when the global performance are still measured. Instead of writing these results through an I/O, it is more advisable to store them in memory and output them at the end of the program execution, when all timing measurements are finished and performance penalties are not important anymore, as shown in Fig.~\ref{fig:instrumentation}. However, read and write operations to main memory can also be slow and hinder the performance when measuring them. This effect can be minimized by using much faster memories such as caches, but such memories are also typically small, shared with the algorithm being measured and required by this algorithm to perform faster. Select carefully the data to be collected in order to reduce the necessary memory to store them and reduce the impact on the algorithm's performance. There is no solution to eliminate all influence on performance when collecting data, but this effect can and should be lowered at most.
% Collect start and stop time, using clock_gettime with its different clocks
% Avoid printf nor I/O in the middle of your experiments, as this is very slow and can artifically reduce the performance measured
% Instead, collect numbers and store them in memory
% Do not store too much data as main memory can also be slow and reduce the performance; consider cache-miss issues
% Select carefully the data you want to measure.

\subsection{Step 3: Set up the different compilation and runtime settings to be used}
\label{sec:plan}
Freja uses the content of \emph{variables} to build the complete list of possible versions the program can have and that must be compiled. This list is also used to run and collect data from all possible versions. The settings' names and the values they can take are written using the syntax for Bash scripts. As discussed in Sec.~\ref{sec:setup}, settings' values can be given as programs' runtime arguments or preprocessor directives resolved at compilation time. This distinction must be made when filling settings' names and possible values in \emph{variables}. Appendix~\ref{app:variables} gives a complete example of this file. Several items need to be filled:
\begin{itemize}
\item output\_columns\\
Describes the meaning of each column of the matrix the program outputs when sending the data it measured (one word per column). Elements are separated by a space and the list must be surrounded by double quotes(").
\item compile\\
List of the settings whose value is given at compilation time to the {C} preprocessor. All elements are separated by a space and the list is surrounded by \emph{parentheses} only (no double quotes).
%Elements of this list cannot take some name such as \emph{count}, as these names are used in other parts of the scripts and their use can randomly alter the good functioning of the scripts. There is no exhaustive list of these forbidden values, so users are encouraged to report them.
\item run\\
List of the settings whose value is given at runtime to the program as an argument. This field has the same limitations and constraints as \emph{compile}. The names used in \emph{compile} and \emph{run} are mutually exclusive.
\item restrict\\
Boolean expression that is checked for each possible setting combination and that returns 0 if the combination instance must be run, and non-zero otherwise. The value of a setting can be checked through the built-in function \emph{match(var, "expr")} that takes as parameter \emph{var}, the name of a setting as listed in the lists \emph{compile} or \emph{run}, and \emph{"expr"}, a regular expression interpreted by sed\footnote{https://www.gnu.org/software/sed/manual/sed.html} and surrounded by double quotes. \emph{match(var, "expr")} returns 0 if a setting combination instance yields \emph{"expr"} to match the value of \emph{var}.
\end{itemize}
Every element in \emph{run} and \emph{compile} lists must be defined as a variable, taking as values the list of all possible value this setting can take. This list must be surrounded by double quotes.

In this scenario, we define as compile-time settings the number of threads to use as well as the amount of randomness of work the program may have to do (respectively settings \emph{nb\_threads} and \emph{entropy}. Run-time settings include the overall number of jumps the program needs to perform (\emph{count}) as well as the number of run each combination should be run. Note in Appendix~\ref{app:variables} the use of backquotes ``\`'', that is in bash whatever the command between the quotes returns. Here, a sequence between 2 and 8 for \emph{nb\_threads}. We also define in \emph{output\_columns} all columns we expect our program to produce. Finally, for the sake of illustrating the use of the restrict item, we restrict the run of settings combinations to the ones that do not run 3 threads at the second run.

For each setting and setting instance listed in \emph{compile} or \emph{run}, the user can define a friendly label to appear in final graphs generated in the final step 6. Labels are defined as shown in Listing~\ref{lst:labels} (that is a sample of Appendix~\ref{app:variables}), where the labels for all instances of a setting are defined in the variable \emph{labels\_}$<setting's name>$. Here, \emph{seq.} holds for a sequential version of the program under test, \emph{over.} is a parallel implementation that uses only one thread (but pays the overhead price of thread spawning). These two setting instances are labelled ``Sequential'' and ``Overhead'', respectively. In a label definition, pairs of settings' instance name and label are separated with a semicolon (``;'') and elements of a pair are delimited with a colon (``:''). Note the labels for columns, that also provide friendly labels for each setting class.

\begin{lstlisting}[caption={Define human-friendly labels for each setting and setting instance.},label={lst:labels}]
labels_nb_threads="seq.: Sequential; over.: Overhead"
labels_count="100000000:10^8; 200000000: 20^8"
labels_thread="0:Thread 1 (seq.); 1: Thread 1; 2: Thread 2"
labels_columns="nb_threads: Number of threads; \
	count: Number of iterations; thread: Thread;"
\end{lstlisting}

\subsection{Step 4: Compile your program}
\label{sec:compile}
It is a good practice to provide a Makefile to build your program. The syntax of a Makefile is not described in this document. Refer to make's manual\footnote{http://www.gnu.org/s/make/} for this, or browse the internet for examples. Section~\ref{app:makefile} gives the one used in this case study and Fig.\ref{lst:makefile} shows the important lines in the context of this section. What is important to know here is that make and Makefiles can take arguments of the form $argument=value$ when invoking the make tool. This argument is interpreted as a variable taking this value in the Makefile. Listing~\ref{fig:makefile} shows that gcc is called in order to compile program.c to a binary output whose name depends on a suffix given when make is invoked. GCC also takes as arguments {CFLAGS}, that defines the constant symbols {MEASURE}, {NB\_THREADS} and {ENTROPY}. The two latter symbols take respectively the value of the makefile's variables {NB\_THREADS} and {ENTROPY}, which are given as arguments when invoking make. {CFLAGS} also includes switches such as {-Wall} that forces a more strict control on the code and {-O0} which disables every optimizations the compiler could provide. The former flag is a good programming practice while the latter is important to make sure that no unexpected optimization can alter the behavior of the code, thus allowing more accurate performance measurements. {-g} and other flags enable debugging and provide with the linker the libraries that the program has to be linked with.

\begin{lstlisting}[caption={GCC is invoked with options defining symbols handled by the {C} preprocessor directives (\#if, etc).},label={lst:makefile},language=make]
CFLAGS=-g -O0 -Wall -lrt -pthread -DMEASURE\t
 -DNB_THREADS=$(NB_THREADS) -DENTROPY=$(ENTROPY)
[...]
	gcc $(CFLAGS) -o program$(SUFFIX) program.c
\end{lstlisting}

More focus in this case study is given to the file \emph{compile}, as this file is called once per variant to be compiled with the list of compilation settings and one possible combination given as arguments. A complete version of this file written in Bash is given in appendix\ref{app:compile} and its most important parts are illustrated in Listing.\ref{lst:compile}. The arguments are given in the same order as the list described in section~\ref{sec:plan}. In this case study, \$1 defines the entropy and in \$2 is given the number of threads that have to be compiled in this version. It is a good practice to assign these values to variables whose name is comprehensive. This clarifies what information is used in every line of the script, and facilitates the propagation of changes from \emph{variables} to \emph{compile}. Listing~\ref{lst:compile} shows that make is called with arguments defining {ENTROPY} to the value of the entropy setting for this compilation as well as the number of threads to use. At the binary output's file name is appended the value of entropy and number of threads. Having a separate name for each compiled program variants prevents variant from being overwritten by each other, and makes it possible to start the right version when a particular measurement is to be started. Notice \emph{compile} must return a value so the global batch knows if it succeed or not. This is achieved with \emph{exit <value>} where \emph{<value>} is the value to return. 0 denote success and anything else report a failure. A convenient way to obtain a suitable value consists in reading \emph{\$?}, which returns same value as the last program or command called, usually having the same convention.

\begin{lstlisting}[caption={\emph{compile} catches the values of compilations settings and passes them to make.},label={lst:compile},language=bash]
entropy=$1
nb_threads=$2

make ENTROPY=$entropy NB_THREADS=$nb_threads\
 SUFFIX=-$entropy-$nb_threads
\end{lstlisting}

Once the files \emph{variables} and \emph{compile} are ready, the compilation process is started with the command \emph{freja compile}.

\subsection{Step 5: Run your experiments}
\label{sec:run}
All algorithm's variants are run through the bash script \emph{run} that, similarly to \emph{compile} (see section~\ref{sec:compile}) takes as arguments one possible combination of all the settings each time it is called. An example of this file is given in appendix~\ref{app:run} and illustrated in Listing.~\ref{lst:run}. The compilation settings are given first then the runtime settings are appended, both taken from \emph{variables}. In this example, arguments 1 through 2 are taken from the compilation settings and arguments 3 and 4 represent the runtime settings. The binary file whose file name has for suffix the matching entropy value and number of threads is executed with the runtime argument ct, that defines the number of jumps the program has to perform. The parameter \emph{try} is never used and stands only to run several times (defined in \emph{variables} as 1 to 3) the same variant.

\begin{lstlisting}[caption={\emph{run} catches the compilation settings first, then the run settings and run the right binary with relevant parameters.},label={lst:run},language=bash]
#!/bin/bash -f

# compilation settings
entropy=$1
nb_threads=$2

# run settings
ct=$3
try=$4

./program-$entropy-$nb_threads $ct

exit $?
\end{lstlisting}

In order to make Freja to catch their output from the measurements, \emph{run} must absolutely print on standard output the matrix generated by the program variant, regardless of the way it outputs the data collected. \emph{run} must output this matrix, \emph{and nothing else}, to standard output. On this example, the program variant directly prints it on the terminal. It may also display data files the program would generate, after some possible filtering. As for \emph{compile} script (see section~\ref{sec:compile}), it is very important the \emph{run} script returns 0 if the experiment was successful or another value in case of failure. The use of the keyword \emph{exit} and special variable \emph{\$?} is suitable to this purpose.

% Very rough time estimation algorithm
% Give a name to your experiment; if you run it again with more settings but the same code, the data will be merged together
Once the file \emph{variables} is ready and the compilation process is finished, all program variants are run and by the command \emph{freja run $<name>$}, where $<name>$ is a name to the experiment being conducted. The script catches the data outputted on standard output, and stores it into the file \emph{$<name>$/table.csv}. The batch script tries to give a very rough estimation of the remaining time before the batch is finished. This evaluation is based on the time that was necessary to run the program versions already run. It is therefore very rough and shouldn't be used to estimate the perfomance of the program monitored. If a variant run did not work properly and \emph{run} reported this failure through an appropriate exit value, then freja records its output but it does not integrate it into the final data output. As a consequence, failures do not necessarily jeopardize the whole measurement process, since their output are identified and ignored (provided of course, that the script \emph{run} exits with a non-zero value in case of failure). The faulty variant can be fixed and run in a later batch using the same experiment name. Alternatively, since incorrect output is not entirely discarded but only commented, the user can still collect it manually and analyze it if necessary.

If two or more experiments are successively run with the same name, their results are merged together into a single file \emph{table.csv}. Every experiment runs separately stores its result in a specific output file. The results of two experiments of the same name can be merged again to \emph{table.csv} using the command \emph{freja merge $<name>$}. Two different experiments, that differ in features that are not defined by any setting in \emph{variables} can be merged together by the command \emph{freja fuse $<column>$ $<res1.csv>$ $<res2.csv>$ -- output <$output\_file>$}, to the file $<output\_file>$, or to \emph{frame.csv} if no output file is specified. In this file, data coming from one or another experiment can be identified thanks to an additional column (whose name is given by \emph{$<column>$}) inserted before the first column of the original data. It takes as value the index of the arguments given to \emph{merge} that referred to its original result file, starting at 0. For instance, in the output of \emph{freja merge exp1.csv exp2.csv}, all data found in exp1.csv will be prefixed by 0 and all data coming from exp2.csv is prefixed by 1 in the output merged file. It is the user's responsibility to make sure that all files merged together hold data with the same structure; in particular, all data should be organized using the exact same column set, ordered in the same way.

\subsection{Step 6: Process and plot the data collected}
\label{sec:format}
Once data generated through previous steps 1 through 5 is available in a CSV (Comma-Separated Values) file (typically \emph{table.csv}), it is necessary to analyse it to extract and show all relevant information. In this scenario, we are interested in showing the execution time of our program as a function of number of threads, for different problem sizes. The following sections introduces the approaches available for R and Matlab/Octave to produce and regenerate graphs after the data previously generated, or another set of data regenerated through step 1 to 5 in a later experiment.

\subsubsection{Using R}
This section uses R to analyze the data produced in steps 1 through 5, relying on the \emph{ggplot2} to produce visual plots. This section doesn't give any introduction to R, as many introductory and example documents already exist on the web~\cite{r,rdebuts,ggplot2,ggplot2_examples}. Instead, this section reviews the specific use of R's capabilities to our scenario and shows how to apply labels defined in step 3 (Sec.~\ref{sec:plan}).

First, Listing~\ref{lst:r_intro} shows a typical header for a R script to use with Freja. The script loads the packages ggplot2 and plyr, that are used for plotting and analyzing data, respectively. The data set is read from the csv-formatted input file \emph{table.csv'} produced by Freja. Finally, it loads a r script \emph{labels.r}, also generated by Freja, that reflects the human-readable labels defined in Step 3 (Sec.~\ref{sec:plan}).
\begin{lstlisting}[caption={Introductory section of a R script to plot data produced by Freja.},label={lst:r_intro},language=r]
## Load packages
library(ggplot2)
library(plyr)

## Read the data set
data.frame = read.csv("table.csv")

## Load labels and labeling functions
source("labels.r")
\end{lstlisting}

Listing~\ref{lst:r_process} shows how R can be used to define a function to convert nanoseconds returned by calls of \emph{clock\_gettime()}, and how this function is used to compute mean and standard deviation of global and per-thread execution time. Mean and standard deviations are computed with regards to the number of threads (\emph{nb\_threads}), the thread number (\emph{thread}) and the problem size (\emph{count}). Basically, global and per-thread execution time are averaged over all running repetitions defined by the setting \emph{try}.

\begin{lstlisting}[caption={R code to convert the time format returned by calls to clock\_gettime() into single numeric values in seconds, and computation of average and standard deviations of global and per-thread execution time.},label={lst:r_process},language=r]
## Define a function to convert nanoseconds to seconds
nsec2sec = function(nsec)
{
  return(nsec / 1000000000)
}

## Compute the execution time for each run of the
## experiment
data.frame = ddply(
  data.frame, c("nb_threads", "thread", "count",
  	"try"), summarize, thread_time = 
    thread_stop_sec + nsec2sec(thread_stop_nsec) -
    thread_start_sec - nsec2sec(thread_start_nsec),
  global_time = 
    stop_time_sec + nsec2sec(stop_time_nsec) -
    start_time_sec - nsec2sec(start_time_nsec)
)

## Compute mean and standard deviation
data.frame = ddply(
  data.frame, c("nb_threads", "thread", "count"),
  summarize, thread_mean_time = mean(thread_time),
  global_mean_time = mean(global_time),
  thread_time_std = sd(thread_time),
  global_time_std = sd(global_time)
)
\end{lstlisting}

The last phase of the R script uses ggplot2 and the data generated and processed in the previous steps and phases to generate meaningful scripts. You can browse the internet for example and documentation about ggplot2. Here, Freja provides through \emph{labels.r} a convenient way to translate every values and column names to labels as defined in Step 3 (Sec.~\ref{sec:plan}). Notice that ggplot is not provided directly with the data frame processed in earlier phases, but through the function \emph{apply\_labels()}, that translates every values in the data frame with their labels. Similarly, columns's names used as axis labels (here \emph{nb\_threads}) are translated using the function \emph{label()}. It is a good practice apply labels only after all data has been processed, when plotting, as doing so at any earlier phase modifies the frame's key values and invalidates conditions that may apply when processing the data.

\begin{lstlisting}[caption={Use of ggplots to produce a simple dotted curve showing the execution time as a function of number of threads, for problem sizes ranging from $10^8$ to $20^8$ jumps.},label={lst:r_plot},language=r]
## Create a simple plot with ggplots
plot = ggplot() +
  geom_line(data = apply_labels(data.frame),
            aes(nb_threads, global_mean_time,
            group = count, color = count),
            size=1) +
  geom_point() +
  guides(fill=guide_legend(title="Thread"),
  	colour = guide_legend(title="Problem size"),
  	point=guide_legend(title="Point")
  ) +
  ylab("Running time in seconds") +
  xlab(label("nb_threads")) +
  ggtitle("Running time for empty loops") + 
  scale_fill_manual(values = colorRampPalette(
  	c("#FF0000", "#000000"))(9)
  ) +
  scale_colour_manual(
  	values = c("#771C19", "#B6C5CC")
  	)
## Save the plot as a svg file
ggsave(file="1_timing.svg", plot=plot, width=8, height=6)
\end{lstlisting}

\subsubsection{Using matlab/Octave}
The data generated by Freja in CSV format is unsuitable for Matlab or Octave. Freja can convert a csv file into its Matlab/Octave equivalent, using the experiment definition in file \emph{variables} (Sec.~\ref{sec:plan}). The converted data associates in a cell a numeric matrix and descriptive names for each columns and values of the matrix. Matlab or Octave can load directly this generated data and manipulate it. In order to ease the manipulation, several functions inspired from relational algebra are provided and data fields are addressable through their names. A complete commented example for this case study is available in the file \emph{drawplots.m}. This script run by Matlab or Octave collects the data and processes it to obtain specific results used to plot graphs. It relies mainly on a set of data manipulation routines shortly described below. This script is usually very dense regarding the meaning of its lines, which makes it difficult to write correctly or to find the source of its defects. It usually takes shape of three parts: the first part collects the data, filters it through \emph{select} and \emph{where} operations and transforms it by applying custom functions. In this example such functions take recorded start and stop times (global and per thread) through their values in seconds and nanoseconds and compute the time in millisecond between these two steps in the program run. The second part extracts and reshapes this data in order to obtain small matrices ready to be plot. Finally, the third part uses the plot functions to generate and format graphs, and stores them in graphic files.

Below is a very short description of the functions available. More extensive documentation is available in the files where these functions are implemented:
\begin{itemize}
\item select\\
Filters the input matrix and keeps only the columns whose indexes are given as parameters (select.m).
\item duplicate\\ 
Duplicates once or more one or several columns of the input matrix.
\item where\\
Select matrices' row that fulfills conjunctive and/or disjunctive conditions (where.m)
\item apply\\
Applies a custom function and writes its result to the columns which index is given in arguments (apply.m).
\item groupby\\
Separates and groups rows by the content of the columns whose index is given as argument (groupby.m)
\item reduce\\
Reduces groups produced by groupby to one row per group, using reduction functions such as mean, standard deviation or custom functions. Merges rows to a matrix (reduce.m)
\item extend\\
Makes sure every groups has the same number of rows and inserts new rows with suitable data if necessary (extend.m)
\item cellfindstr\\
Finds and return the index of a string in a cell of strings. Returns 0 if the string could not be found
\item coln\\
Returns all column names of a table
\item data\\
Returns data matrix contained in the table
\item insert\\
Inserts and add a matrix into a table
\item setc\\
Replaces all column names in a table with names given as argument
\item setd\\
Replaces data matrix in table with data given as argument
\end{itemize}

Matrices can be plotted using the following plotting routines:
\begin{itemize}
\item quickplot\\
Plots one or more curves into a graph and displays axis labels, curves legend and graph title in a flexible way (quickplot.m)
\item quickerrorbar\\
Same as quickplot, but also allows the plotting of error bars (quickerrorbar.m)
\item quickbar\\
Plots a histograms with one or more bars per element in the x axis (quickbar.m)
\item quickgantt\\
Draws a gantt diagram, one horizontal line per element (quickgantt.m)
\end{itemize}

The command \emph{octave demo.m} runs the octave script named \emph{demo.m} (that runs \emph{drawplots.m}) and generates the graphs. Matlab cannot run the script in the same way. Open Matlab and browse in the left frame until you reach the folder where the results and all the scripts are stored, then start the script ``demo.m''. Note that all these functions were written for Octave, that has a more permissive syntax especially when addressing subsets of cells returned by functions or subexpressions. As Matlab doesn't allow these constructions, some adaptation may be required.

\section{Conclusion}
\label{sec:conclusion}
The example used as a case study in this document produces a graph comparable to the one shown in Fig.\ref{fig:timing-200}. Its shows the time to perform 200 millions jumps using a sequential loop or splitting it between 1 to 8 threads. The plots clearly show that the global time decreases with the number of threads and it shows that the computation load is distributed equally among the threads. Finally, the low difference between the sequential and the parallel version using one thread shows that the overhead of parallelization is negligible compared to the overall performance.

This documents describes a general methodology to manage easily the run of several variants of a single algorithm implementation, and the collection, processing and representation of collected data. This is a tedious task that the tools described here make both easier and faster. Although they considerably reduce the occasions to commit mistakes in experiments due to human interventions, they do not eliminate such mistake. One possible remaining pitfall lies in the data collection method (the choice of the clock) and the parts that could not be automated. A too big confidence in this automated process may hinder cautions to take when performing the remaining manual interventions, or even simply makes the user to forget these steps even if they may be important. More efforts need to be done to improve the set of scripts described here, such as providing them with the ability to interrupt and resume an experiment. Contributions are encouraged, both through coding more functionality or bug reports.

\clearpage
\appendix
\section{Program to monitor performance}
\label{sec:files}
\subsection{Sequential version}
\label{app:sequential}
\lstinputlisting[language=C]{../demo/program_seq.c}

\subsection{Multithreaded version}
\label{app:parallel}
\lstinputlisting[language=C]{../demo/program_par.c}

\subsection{Unified sequential and multithreaded versions}
\label{app:unified}
\lstinputlisting[language=C]{../demo/program_both.c}

\section{Fully instrumented C program}
\label{app:instrumented}
\lstinputlisting[language=C]{../demo/program.c}

\section{Variables}
\label{app:variables}
\lstinputlisting[language=bash]{../demo/variables}

\section{Compilation}
\label{app:compile}
\lstinputlisting[language=bash]{../demo/compile}

\subsection{Makefile}
\label{app:makefile}
\lstinputlisting[language=make]{../demo/Makefile}

\section{Run script}
\label{app:run}
\lstinputlisting[language=bash]{../demo/run}






